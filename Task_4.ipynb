{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4 - Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as T\n",
    "import keras as keras\n",
    "from keras import backend as K\n",
    "from keras import initializers\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.models import Sequential, Model, load_model, save_model\n",
    "from keras.layers.core import Dense, Lambda, Activation\n",
    "from keras.layers import Embedding, Input, Dense, merge, Reshape,  Flatten, Dropout\n",
    "from keras.optimizers import Adagrad, Adam, SGD, RMSprop, Adamax\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Multiply, Concatenate\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "import sys\n",
    "import math\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "RATING_DATA_TEST_FILE = \"u1.test\"\n",
    "RATING_DATA_TRAIN_FILE = \"u1.base\"\n",
    "MODEL_WEIGHTS_FILE = \"u_emb_weights.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  movie_id  rating  timestamp\n",
      "0        1         1       5  874965758\n",
      "1        1         2       3  876893171\n",
      "2        1         3       4  878542960\n",
      "3        1         4       3  876893119\n",
      "4        1         5       3  889751712\n"
     ]
    }
   ],
   "source": [
    "#import of the data\n",
    "\n",
    "m_cols = ['user_id','movie_id','rating','timestamp']\n",
    "\n",
    "df_train = pd.read_csv(RATING_DATA_TRAIN_FILE, sep='\\t', engine='python', encoding='latin-1',names=m_cols)\n",
    "df_test = pd.read_csv(RATING_DATA_TEST_FILE, sep='\\t', engine='python', encoding='latin-1',names=m_cols)\n",
    "\n",
    "print(df_train.head())\n",
    "\n",
    "max_userid = df_train['user_id'].drop_duplicates().max()\n",
    "max_movieid = df_train['movie_id'].drop_duplicates().max()\n",
    "df_train['user_emb_id'] = df_train['user_id'] - 1\n",
    "df_train['movie_emb_id'] = df_train['movie_id'] - 1\n",
    "df_test['user_emb_id'] = df_test['user_id'] - 1\n",
    "df_test['movie_emb_id'] = df_test['movie_id'] - 1\n",
    "\n",
    "\n",
    "Train_Users = df_train['user_emb_id'].values\n",
    "Train_Movies = df_train['movie_emb_id'].values\n",
    "Train_Ratings = df_train['rating'].values\n",
    "\n",
    "Test_Users = df_test['user_emb_id'].values\n",
    "Test_Movies = df_test['movie_emb_id'].values\n",
    "Test_Ratings = df_test['rating'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ncf_model1(num_users, num_items, latent_dim,hidden_dim,do):\n",
    "    # Input variables\n",
    "    user_input = Input(shape=(1,), dtype='int32', name = 'user_input')\n",
    "    item_input = Input(shape=(1,), dtype='int32', name = 'item_input')\n",
    "\n",
    "    NCF_Embedding_User = Embedding(input_dim = num_users, output_dim = latent_dim, name = 'user_embedding', input_length=1)\n",
    "    NCF_Embedding_Item = Embedding(input_dim = num_items, output_dim = latent_dim, name = 'item_embedding', input_length=1)   \n",
    "    \n",
    "    # Crucial to flatten an embedding vector!\n",
    "    user_latent = Flatten()(NCF_Embedding_User(user_input))\n",
    "    item_latent = Flatten()(NCF_Embedding_Item(item_input))\n",
    "    \n",
    "    # Element-wise product of user and item embeddings\n",
    "    conc = Concatenate()([user_latent, item_latent])\n",
    "    drop = Dropout(0.3)(conc)\n",
    "    hid1 = Dense(hidden_dim, activation='relu')(conc)\n",
    "    drop2  = Dropout(do)(hid1)\n",
    "    prediction = Dense(1, activation='relu', kernel_initializer='lecun_uniform', name = 'prediction')(drop2)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[user_input, item_input], outputs=prediction)\n",
    "    print(\"ncf model1\")\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ncf model1\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "user_input (InputLayer)         (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_input (InputLayer)         (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_embedding (Embedding)      (None, 1, 20)        18860       user_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "item_embedding (Embedding)      (None, 1, 20)        33640       item_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 20)           0           user_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 20)           0           item_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 40)           0           flatten_7[0][0]                  \n",
      "                                                                 flatten_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 20)           820         concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 20)           0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "prediction (Dense)              (None, 1)            21          dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 53,341\n",
      "Trainable params: 53,341\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K_LATENT = 20\n",
    "hidden_dim = 20\n",
    "do = 0.3\n",
    "NCF_model1 = get_ncf_model1(max_userid,max_movieid,K_LATENT,hidden_dim,do)\n",
    "NCF_model1.compile(loss='mse',optimizer=Adamax(),metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 64000 samples, validate on 16000 samples\n",
      "Epoch 1/100\n",
      "64000/64000 [==============================] - 2s 36us/step - loss: 2.2341 - mae: 1.1451 - val_loss: 1.2401 - val_mae: 0.9260\n",
      "Epoch 2/100\n",
      "64000/64000 [==============================] - 2s 35us/step - loss: 1.1499 - mae: 0.8566 - val_loss: 1.1022 - val_mae: 0.8646\n",
      "Epoch 3/100\n",
      "64000/64000 [==============================] - 2s 35us/step - loss: 1.0460 - mae: 0.8157 - val_loss: 1.0861 - val_mae: 0.8577\n",
      "Epoch 4/100\n",
      "64000/64000 [==============================] - 2s 35us/step - loss: 0.9783 - mae: 0.7876 - val_loss: 1.0796 - val_mae: 0.8547\n",
      "Epoch 5/100\n",
      "64000/64000 [==============================] - 2s 36us/step - loss: 0.9365 - mae: 0.7698 - val_loss: 1.0612 - val_mae: 0.8458\n",
      "Epoch 6/100\n",
      "64000/64000 [==============================] - 2s 34us/step - loss: 0.9122 - mae: 0.7588 - val_loss: 1.0545 - val_mae: 0.8424\n",
      "Epoch 7/100\n",
      "64000/64000 [==============================] - 2s 34us/step - loss: 0.8894 - mae: 0.7480 - val_loss: 1.0555 - val_mae: 0.8433\n",
      "Epoch 8/100\n",
      "64000/64000 [==============================] - 2s 34us/step - loss: 0.8746 - mae: 0.7398 - val_loss: 1.0595 - val_mae: 0.8471\n",
      "Epoch 9/100\n",
      "64000/64000 [==============================] - 2s 36us/step - loss: 0.8643 - mae: 0.7341 - val_loss: 1.0501 - val_mae: 0.8409\n",
      "Epoch 10/100\n",
      "64000/64000 [==============================] - 2s 35us/step - loss: 0.8557 - mae: 0.7298 - val_loss: 1.0403 - val_mae: 0.8374\n",
      "Epoch 11/100\n",
      "64000/64000 [==============================] - 2s 34us/step - loss: 0.8494 - mae: 0.7268 - val_loss: 1.0504 - val_mae: 0.8441\n",
      "Epoch 12/100\n",
      "64000/64000 [==============================] - 2s 34us/step - loss: 0.8447 - mae: 0.7248 - val_loss: 1.0548 - val_mae: 0.8453\n",
      "Epoch 13/100\n",
      "64000/64000 [==============================] - 2s 35us/step - loss: 0.8425 - mae: 0.7242 - val_loss: 1.0541 - val_mae: 0.8466\n",
      "Epoch 14/100\n",
      "64000/64000 [==============================] - 2s 38us/step - loss: 0.8388 - mae: 0.7220 - val_loss: 1.0434 - val_mae: 0.8393\n",
      "Epoch 15/100\n",
      "64000/64000 [==============================] - 3s 40us/step - loss: 0.8356 - mae: 0.7202 - val_loss: 1.0599 - val_mae: 0.8503\n"
     ]
    }
   ],
   "source": [
    "learnTime1 = time.time()\n",
    "callbacks = [EarlyStopping('val_loss', patience=5), ModelCheckpoint(MODEL_WEIGHTS_FILE, save_best_only=True)]\n",
    "history = NCF_model1.fit([Train_Users, Train_Movies], Train_Ratings, epochs=100, validation_split=.2, verbose=1, callbacks=callbacks, batch_size = 32)\n",
    "learnTime1 = time.time() - learnTime1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE1 = 0.749746728515625\n"
     ]
    }
   ],
   "source": [
    "preddict_model1 = NCF_model1.predict([Test_Users,Test_Movies])\n",
    "test_predict1 = pd.DataFrame(data=preddict_model1, columns=['Prediction'])\n",
    "test_predict1['Real_Rating'] = Test_Ratings\n",
    "\n",
    "MAE1 = np.sum(abs(test_predict1['Real_Rating']-test_predict1['Prediction']))/test_predict1.shape[0]\n",
    "                                \n",
    "print(\"MAE1 = \"+ str(MAE1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ncf_model2(num_users, num_items, latent_dim,hidden_dim,do):\n",
    "    # Input variables\n",
    "    user_input = Input(shape=(1,), dtype='int32', name = 'user_input')\n",
    "    item_input = Input(shape=(1,), dtype='int32', name = 'item_input')\n",
    "\n",
    "    NCF_Embedding_User = Embedding(input_dim = num_users, output_dim = latent_dim, name = 'user_embedding', input_length=1)\n",
    "    NCF_Embedding_Item = Embedding(input_dim = num_items, output_dim = latent_dim, name = 'item_embedding', input_length=1)   \n",
    "    \n",
    "    # Crucial to flatten an embedding vector!\n",
    "    user_latent = Flatten()(NCF_Embedding_User(user_input))\n",
    "    item_latent = Flatten()(NCF_Embedding_Item(item_input))\n",
    "    \n",
    "    # Element-wise product of user and item embeddings\n",
    "    conc = Concatenate()([user_latent, item_latent])\n",
    "    drop = Dropout(0.3)(conc)\n",
    "    hid1 = Dense(hidden_dim, activation='sigmoid')(conc)\n",
    "    drop2  = Dropout(do)(hid1)\n",
    "    prediction = Dense(1, activation='sigmoid', kernel_initializer='lecun_uniform', name = 'prediction')(drop2)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[user_input, item_input], outputs=prediction)\n",
    "    print(\"ncf model2\")\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ncf model2\n",
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "user_input (InputLayer)         (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_input (InputLayer)         (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_embedding (Embedding)      (None, 1, 20)        18860       user_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "item_embedding (Embedding)      (None, 1, 20)        33640       item_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 20)           0           user_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 20)           0           item_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 40)           0           flatten_9[0][0]                  \n",
      "                                                                 flatten_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 20)           820         concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 20)           0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "prediction (Dense)              (None, 1)            21          dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 53,341\n",
      "Trainable params: 53,341\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K_LATENT = 20\n",
    "hidden_dim = 20\n",
    "do = 0.3\n",
    "NCF_model2 = get_ncf_model2(max_userid,max_movieid,K_LATENT,hidden_dim,do)\n",
    "NCF_model2.compile(loss='mse',optimizer=Adamax(),metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 64000 samples, validate on 16000 samples\n",
      "Epoch 1/100\n",
      "64000/64000 [==============================] - 2s 39us/step - loss: 7.8455 - mae: 2.5625 - val_loss: 7.7429 - val_mae: 2.5606\n",
      "Epoch 2/100\n",
      "64000/64000 [==============================] - 2s 35us/step - loss: 7.6235 - mae: 2.5212 - val_loss: 7.7360 - val_mae: 2.5592\n",
      "Epoch 3/100\n",
      "64000/64000 [==============================] - 2s 36us/step - loss: 7.6212 - mae: 2.5207 - val_loss: 7.7352 - val_mae: 2.5590\n",
      "Epoch 4/100\n",
      "64000/64000 [==============================] - 2s 36us/step - loss: 7.6209 - mae: 2.5207 - val_loss: 7.7351 - val_mae: 2.5590\n",
      "Epoch 5/100\n",
      "64000/64000 [==============================] - 2s 36us/step - loss: 7.6209 - mae: 2.5207 - val_loss: 7.7351 - val_mae: 2.5590: 7.6248 - mae: 2.52\n",
      "Epoch 6/100\n",
      "64000/64000 [==============================] - 2s 36us/step - loss: 7.6209 - mae: 2.5207 - val_loss: 7.7351 - val_mae: 2.5590\n",
      "Epoch 7/100\n",
      "64000/64000 [==============================] - 2s 36us/step - loss: 7.6209 - mae: 2.5207 - val_loss: 7.7351 - val_mae: 2.5590\n",
      "Epoch 8/100\n",
      "64000/64000 [==============================] - 2s 36us/step - loss: 7.6209 - mae: 2.5207 - val_loss: 7.7351 - val_mae: 2.5590\n",
      "Epoch 9/100\n",
      "64000/64000 [==============================] - 2s 37us/step - loss: 7.6209 - mae: 2.5207 - val_loss: 7.7351 - val_mae: 2.5590\n",
      "Epoch 10/100\n",
      "64000/64000 [==============================] - 2s 37us/step - loss: 7.6209 - mae: 2.5207 - val_loss: 7.7351 - val_mae: 2.5590\n",
      "Epoch 11/100\n",
      "64000/64000 [==============================] - 2s 37us/step - loss: 7.6209 - mae: 2.5207 - val_loss: 7.7351 - val_mae: 2.5590\n",
      "Epoch 12/100\n",
      "64000/64000 [==============================] - 2s 36us/step - loss: 7.6209 - mae: 2.5207 - val_loss: 7.7351 - val_mae: 2.5590\n",
      "Epoch 13/100\n",
      "64000/64000 [==============================] - 2s 38us/step - loss: 7.6209 - mae: 2.5207 - val_loss: 7.7351 - val_mae: 2.5590\n"
     ]
    }
   ],
   "source": [
    "learnTime2 = time.time()\n",
    "callbacks = [EarlyStopping('val_loss', patience=5), ModelCheckpoint(MODEL_WEIGHTS_FILE, save_best_only=True)]\n",
    "history = NCF_model2.fit([Train_Users, Train_Movies], Train_Ratings, epochs=100, validation_split=.2, verbose=1, callbacks=callbacks, batch_size = 32)\n",
    "learnTime2 = time.time() - learnTime2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE2 = 2.5359\n"
     ]
    }
   ],
   "source": [
    "preddict_model2 = NCF_model2.predict([Test_Users,Test_Movies])\n",
    "test_predict2 = pd.DataFrame(data=preddict_model2, columns=['Prediction'])\n",
    "test_predict2['Real_Rating'] = Test_Ratings\n",
    "\n",
    "MAE2 = np.sum(abs(test_predict2['Real_Rating']-test_predict2['Prediction']))/test_predict2.shape[0]\n",
    "                                \n",
    "print(\"MAE2 = \"+ str(MAE2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ncf_model3(num_users, num_items, latent_dim,hidden_dim,do):\n",
    "    # Input variables\n",
    "    user_input = Input(shape=(1,), dtype='int32', name = 'user_input')\n",
    "    item_input = Input(shape=(1,), dtype='int32', name = 'item_input')\n",
    "\n",
    "    NCF_Embedding_User = Embedding(input_dim = num_users, output_dim = latent_dim, name = 'user_embedding', input_length=1)\n",
    "    NCF_Embedding_Item = Embedding(input_dim = num_items, output_dim = latent_dim, name = 'item_embedding', input_length=1)   \n",
    "    \n",
    "    # Crucial to flatten an embedding vector!\n",
    "    user_latent = Flatten()(NCF_Embedding_User(user_input))\n",
    "    item_latent = Flatten()(NCF_Embedding_Item(item_input))\n",
    "    \n",
    "    # Element-wise product of user and item embeddings\n",
    "    conc = Concatenate()([user_latent, item_latent])\n",
    "    drop = Dropout(0.3)(conc)\n",
    "    hid1 = Dense(hidden_dim, activation='relu')(conc)\n",
    "    drop2  = Dropout(do)(hid1)\n",
    "    prediction = Dense(1, activation='relu', kernel_initializer='lecun_uniform', name = 'prediction')(drop2)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[user_input, item_input], outputs=prediction)\n",
    "    print(\"ncf model3\")\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ncf model3\n",
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "user_input (InputLayer)         (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_input (InputLayer)         (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_embedding (Embedding)      (None, 1, 20)        18860       user_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "item_embedding (Embedding)      (None, 1, 20)        33640       item_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_13 (Flatten)            (None, 20)           0           user_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_14 (Flatten)            (None, 20)           0           item_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 40)           0           flatten_13[0][0]                 \n",
      "                                                                 flatten_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 20)           820         concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 20)           0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "prediction (Dense)              (None, 1)            21          dropout_14[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 53,341\n",
      "Trainable params: 53,341\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K_LATENT = 20\n",
    "hidden_dim = 20\n",
    "do = 0.2\n",
    "NCF_model3 = get_ncf_model3(max_userid,max_movieid,K_LATENT,hidden_dim,do)\n",
    "NCF_model3.compile(loss='mse',optimizer=Adam(),metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 64000 samples, validate on 16000 samples\n",
      "Epoch 1/100\n",
      "64000/64000 [==============================] - 3s 40us/step - loss: 1.9284 - mae: 1.0509 - val_loss: 1.9126 - val_mae: 1.1704\n",
      "Epoch 2/100\n",
      "64000/64000 [==============================] - 2s 36us/step - loss: 1.0924 - mae: 0.8330 - val_loss: 1.4087 - val_mae: 0.9937\n",
      "Epoch 3/100\n",
      "64000/64000 [==============================] - 2s 37us/step - loss: 1.0281 - mae: 0.8065 - val_loss: 1.2167 - val_mae: 0.9158\n",
      "Epoch 4/100\n",
      "64000/64000 [==============================] - 2s 37us/step - loss: 0.9789 - mae: 0.7860 - val_loss: 1.0954 - val_mae: 0.8630\n",
      "Epoch 5/100\n",
      "64000/64000 [==============================] - 2s 37us/step - loss: 0.9414 - mae: 0.7712 - val_loss: 1.0819 - val_mae: 0.8568\n",
      "Epoch 6/100\n",
      "64000/64000 [==============================] - 2s 37us/step - loss: 0.9079 - mae: 0.7551 - val_loss: 1.0752 - val_mae: 0.8541\n",
      "Epoch 7/100\n",
      "64000/64000 [==============================] - 2s 38us/step - loss: 0.8844 - mae: 0.7446 - val_loss: 1.0368 - val_mae: 0.8314\n",
      "Epoch 8/100\n",
      "64000/64000 [==============================] - 2s 37us/step - loss: 0.8640 - mae: 0.7343 - val_loss: 1.0419 - val_mae: 0.8357\n",
      "Epoch 9/100\n",
      "64000/64000 [==============================] - 2s 39us/step - loss: 0.8467 - mae: 0.7264 - val_loss: 1.0199 - val_mae: 0.8220\n",
      "Epoch 10/100\n",
      "64000/64000 [==============================] - 3s 40us/step - loss: 0.8342 - mae: 0.7189 - val_loss: 1.0233 - val_mae: 0.8233\n",
      "Epoch 11/100\n",
      "64000/64000 [==============================] - 3s 40us/step - loss: 0.8240 - mae: 0.7156 - val_loss: 1.0097 - val_mae: 0.8152\n",
      "Epoch 12/100\n",
      "64000/64000 [==============================] - 2s 37us/step - loss: 0.8183 - mae: 0.7124 - val_loss: 1.0219 - val_mae: 0.8263\n",
      "Epoch 13/100\n",
      "64000/64000 [==============================] - 2s 35us/step - loss: 0.8102 - mae: 0.7090 - val_loss: 1.0145 - val_mae: 0.8175\n",
      "Epoch 14/100\n",
      "64000/64000 [==============================] - 2s 35us/step - loss: 0.8011 - mae: 0.7050 - val_loss: 1.0277 - val_mae: 0.8270\n",
      "Epoch 15/100\n",
      "64000/64000 [==============================] - 2s 35us/step - loss: 0.7967 - mae: 0.7028 - val_loss: 1.0149 - val_mae: 0.8192\n",
      "Epoch 16/100\n",
      "64000/64000 [==============================] - 2s 35us/step - loss: 0.7904 - mae: 0.6992 - val_loss: 1.0238 - val_mae: 0.8249\n"
     ]
    }
   ],
   "source": [
    "learnTime3 = time.time()\n",
    "callbacks = [EarlyStopping('val_loss', patience=5), ModelCheckpoint(MODEL_WEIGHTS_FILE, save_best_only=True)]\n",
    "history = NCF_model3.fit([Train_Users, Train_Movies], Train_Ratings, epochs=100, validation_split=.2, verbose=1, callbacks=callbacks, batch_size = 32)\n",
    "learnTime3 = time.time() - learnTime3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE3 = 0.74768330078125\n"
     ]
    }
   ],
   "source": [
    "preddict_model3 = NCF_model3.predict([Test_Users,Test_Movies])\n",
    "test_predict3 = pd.DataFrame(data=preddict_model3, columns=['Prediction'])\n",
    "test_predict3['Real_Rating'] = Test_Ratings\n",
    "\n",
    "MAE3 = np.sum(abs(test_predict3['Real_Rating']-test_predict3['Prediction']))/test_predict3.shape[0]\n",
    "                                \n",
    "print(\"MAE3 = \"+ str(MAE3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE1 = 0.749746728515625\n",
      "Running Time: 34.50171446800232\n",
      "\n",
      "MAE2 = 2.5359\n",
      "Running Time: 30.887426614761353\n",
      "\n",
      "MAE3 = 0.74768330078125\n",
      "Running Time: 38.842615365982056\n",
      "\n",
      "The first and third models usually gets the best MAE score, but their learing is usually the longest, their model setup is\n",
      "Model 1\n",
      "activation function: relu\n",
      "loss function: mse\n",
      "optimizer: adamax\n",
      "number of hidden layer: 1\n",
      "Model 3\n",
      "activation function: relu\n",
      "loss function: mse\n",
      "optimizer: adam\n",
      "number of hidden layer: 1 but Dropout is 0.2\n"
     ]
    }
   ],
   "source": [
    "print(\"MAE1 = \"+ str(MAE1))\n",
    "print(\"Running Time: \" + str(learnTime1) + \"\\n\")\n",
    "print(\"MAE2 = \"+ str(MAE2))\n",
    "print(\"Running Time: \" + str(learnTime2) + \"\\n\")\n",
    "print(\"MAE3 = \"+ str(MAE3))\n",
    "print(\"Running Time: \" + str(learnTime3) + \"\\n\")\n",
    "\n",
    "print(\"The first and third models usually gets the best MAE score, but their learing is usually the longest, their model setup is\")\n",
    "print(\"Model 1\")\n",
    "print(\"activation function: relu\")\n",
    "print(\"loss function: mse\")\n",
    "print(\"optimizer: adamax\")\n",
    "print(\"number of hidden layer: 1\")\n",
    "print(\"Model 3\")\n",
    "print(\"activation function: relu\")\n",
    "print(\"loss function: mse\")\n",
    "print(\"optimizer: adam\")\n",
    "print(\"number of hidden layer: 1 but Dropout is 0.2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
